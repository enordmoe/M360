---
title: "Chapter 9:  Logistic Regression"
subtitle: "Sections 9.3-9.4"
date: "Applied Statistics II"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "extra.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width=9, fig.height=4,  
  out.width="90%",
  message = FALSE
)

```



```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(mosaic)
library(Stat2Data)
library(gridExtra)
library(plotly)
library(readr)
trashball <- read_csv("http://people.kzoo.edu/enordmoe/math360/trashball.csv")
data("Putts1")
data("Putts2")
data("MedGPA")

```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_xaringan_extra(c("panelset"))
xaringanExtra::use_clipboard()
xaringanExtra::use_tile_view()
```


```{css echo=FALSE}
.highlight-last-item > ul > li,
.highlight-last-item > ol > li {
  opacity: 0.5;
}
.highlight-last-item > ul > li:last-of-type,
.highlight-last-item > ol > li:last-of-type {
  opacity: 1;
}
```

# Outline 

- Conditions for logistic regression

  - Linearity
  
  - Empirical logit plot
  
- Tests and CIs for slope

- Evaluating overall fit

  - G statistic

  - Disaggregate (long)/Aggregate (short) form in R

---

## Inference Conditions for Logistic Regression

- **Linearity**: The logits (log odds) should have a linear relationship with the predictor.

--

- **Independence**: No pairing or clustering of data.

--

- **Random**: Either a random sample from a population OR random assignment within an experiment.

--

- **~~Normality~~**: This does not apply.

  - The responses are 0/1.

--

- **~~Constant variance~~**: Does not apply.

  - Variability in $Y$ is highest when $\pi$ is near 1/2 lowest when $\pi$ is near 0 or 1.


---

## Checking Linearity for Logistic Regression

### Empirical Logit Plot

1. Find the sample proportion $\hat p$ for each value of the predictor.

2. Plot $\log\left(\frac{\hat p}{1-\hat p}\right)$ vs $x$ and look for a linear trend.

<br>

**Note:** When  the predictor has many values (few repeats),  choose intervals of predictor values and plot the group logits versus the group predictor means.

  - The function `emplogitplot1()` in **Stat2Data** is very useful for constructing these plots.
  
---

## Two Forms of Logistic Daa

1. **Disaggregate (long) form**: Response variable $Y=$ Success/Failure or 1/0 and each case is a row

  - **Binary (Bernoulli) response** logistic regression

  - e.g., **Putts1** data
  
--
  
2. **Aggegrate (short) form**: Response variable $Y=$ Count of Successes for a group of data with a common $X$ value

  - **Binomial counts** logistic regression
  
  - e.g., **Putts2** has 5 cases for each distance of putt

--

<br> 

**Note:**The aggregate form simplifies construction of empirical logit plots.

---

### Empirical Plots Using Putts2 Data

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
Putts2 <- mutate(Putts2, LogitMade = log(Made/Missed))
lmod_Putts2 <- glm(cbind(Made, Missed) ~ Length, family = binomial, data = Putts2)
mymod_fun <- makeFun(lmod_Putts2)
gf_point(LogitMade ~ Length, data = Putts2) %>%
  gf_coefline(model = lmod_Putts2, color = "red")
```
]

---

### Empirical Logit Plot Using Putts1

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
# Automate plot with emplogitplot1() function in Stat2Data
# 
emplogitplot1(Made ~ Length, data = Putts1, ngroups = "all")
```
]


---

## Empirical Logit Plots

### Two possible problems:

1. Typically, there may be many different x-values with few (or only one) cases at each.
  
  - Solution: Group the predictor values into bins of similar values and plot vs. the mean predictor for each bin.
  
2. A sample proportion could be 0 or 1 (making log(odds) undefined).
  
  - Solution: Define adjusted proportion as
    
    $$\hat p_\text{adj}=\frac{\#\text{Yes}+1/2}{\#\text{Yes}+\#\text{No}+1}$$

---

### Empirical Logit Plot for MedGPA Data

There are 15 different MCAT scort values (18 to 48)

  - Use 5 ad hoc groups (18-30), (31-34), (35-38), (39-42), (43-48)

```{r, eval = FALSE}
emplogitplot1(Acceptance ~ MCAT, breaks = c(0, 30, 34, 38, 42, 48), data = MedGPA, out = TRUE)
```

```{r, echo = FALSE}
emplogitplot1(Acceptance ~ MCAT, breaks = c(0, 30, 34, 38, 42, 48), data = MedGPA, out = TRUE, showplot = FALSE)
```

---

### Empirical Logit Plot for MedGPA Data

```{r, echo = FALSE}
emplogitplot1(Acceptance ~ MCAT, breaks = c(0, 30, 34, 38, 42, 48), data = MedGPA)
```

---

### Empirical Logit Plot for MedGPA Data

Specify number of groups and R picks intervals to make group size (rougly) equal

```{r, eval = FALSE}
emplogitplot1(Acceptance ~ MCAT, ngroups = 5, data = MedGPA, out = TRUE)
```

```{r, echo = FALSE}
emplogitplot1(Acceptance ~ MCAT, ngroups = 5, data = MedGPA, out = TRUE, showplot = FALSE)
```


--

<br>

**Note:** Means for $X$ values are the actual means of those in the group, not the midpoint of the interval.

---

class: center, middle, inverse

# Formal Inference: Tests and Intervals

---

### Logistic Output for Putting Example (Putts1)

```{r}
lmod_putts <- glm(Made ~ Length, family = binomial, data = Putts1)
summary(lmod_putts)
```

---

### Logistic Output for Putting Example (Putts1)

- What are the the $z$-tests?

- What is the deviance about?

- Is there a test like the overall $F$-test?

---

### Recall: Ordinary Linear Regression Output

```{r}
regmod_putts <- lm(Made ~ Length, data = Putts1)
summary(regmod_putts)
```

???

- $t$-Tests for individual coefficients

- Residual standard error and $R^2$ to compare models

- $F$-test for overall fit


---

### Waldâ€™s $z$-Test for Individual Coefficients

**Hypotheses**

- $H_0:\beta_i=0$ versus $H_a:\beta_i\neq 0$
  
--

**Test statistic**

- $\displaystyle z=\frac{\hat\beta_i}{SE(\hat\beta_i)}$

--

**P-value**

- $P$-value $=2P(Z>|z|)$

---

### Confidence Intervals for Slope and Odds Ratio

- For simple logistic regression, use the SE to find a confidence interval for $\beta_1$:

  - Calculate $\hat\beta_i\pm z^*\cdot\text{SE}$

---

## Example: Putting Data

**Logistic Model Output**

```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.25684    0.36893   8.828   <2e-16 ***
Length      -0.56614    0.06747  -8.391   <2e-16 ***
```

**Calculations**

- **CI for slope**: $-0.566 \pm 1.96(0.06747) = (-0.698, -0.434)$

- **CI for the odds ratio $(e^{\hat\beta_1})$:** exponentiate the CI for $\beta_1$

  - **CI for OR:** $(e^{-0.698}, e^{-0.434})=(0.497,0.648)$
  
- **Interpretation** in context:

  - "We are 95% confident that the **odds of making** a putt decrease by a factor between 0.50 and 0.65 for every extra foot in length."
  
---

### R commands for CI for Slope and Odds Ratio

**CIs for coefficient**

```{r}
confint(lmod_putts)
```

<br>

**CIs for Odds Ratio**

```{r}
exp(confint(lmod_putts))
```


---

class: center, middle, inverse

# Estimating Parameters in Logistic Regression

---

## Estimating Parameters in Logistic Regression

- **Maximum Likelihood Estimation**: Parameters are chosen to maximize the *likelihood* of the observed sample.

- **The likelihood function:** $$L=\prod \hat\pi_i^{y_i}(1-\hat\pi)^{1-y_i}$$

  - If the $i$th data point is YES $(y_i=1)$, calculate $\hat\pi_i$

  - If the $i$th data point is NO $(y_i=0)$, calculate $1-\hat\pi_i$

<br>

- Want $L$ to be as large as possible

  - Coefficients of the logistic regression model are chosen to make $\log(L)$ as large as possible
  
  - $\hat\beta_1$ is the **maximum likelihood estimator** of $\beta_1$
  
---

### Residual Deviance: R Output

- In assessing the overall fit, we often look at $-2\log(L)$, labeled as the **residual deviance** in R.

```
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.25684    0.36893   8.828   <2e-16 ***
Length      -0.56614    0.06747  -8.391   <2e-16 ***
---

    Null deviance: 800.21  on 586  degrees of freedom
Residual deviance: 719.89  on 585  degrees of freedom
AIC: 723.89
```

- For the **Putts1** data, $-2\log(L)=-2\cdot(-359.95)=719.9$

```{r}
# Extract log-likelihood but we never need to do this
logLik(lmod_putts)
```


- **Usage note:** We want deviance to be small (like SSE of linear regression).


---

### Example: Predict Acceptance for MedGPA Data

**Using GPA**

```{r, eval = FALSE}
lmmod_gpa <- glm(Acceptance ~ GPA, family = binomial, data = MedGPA)
summary(lmmod_gpa)
```

```
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -19.207      5.629  -3.412 0.000644 ***
GPA            5.454      1.579   3.454 0.000553 ***
---
    Null deviance: 75.791  on 54  degrees of freedom
Residual deviance: 56.839  on 53  degrees of freedom
```



**Using MCAT**

```{r, eval = FALSE}
lmmod_mcat <- glm(Acceptance ~ MCAT, family = binomial, data = MedGPA)
summary(lmmod_mcat)
```

```
            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -8.71245    3.23645  -2.692  0.00710 **
MCAT         0.24596    0.08938   2.752  0.00592 **
---
    Null deviance: 75.791  on 54  degrees of freedom
Residual deviance: 64.697  on 53  degrees of freedom
```

---

### Example: Predict Acceptance for MedGPA Data

- We prefer the GPA model with:

  - Lower **Residual deviance** (56.84)
  
  - Greater **reduction in deviance**: (75.78-64.70) vs (75.78-56.84) 
  
---
  
### Evaluating Overall Fit: Drop in Deviance Test

- Test for overall fit (simlilar to regression ANOVA)

  - $G=$ improvement in $-2\log(L)$ over a model with constant only (null deviance) 
  
  - Compare to $\chi^2$ with $k$ degrees of freedom (chi-square)
  
  - Obtain $p$-value from R or by using the `pchisq()` function.
  
---

###  Drop in Deviance (G) Test: Putts1 data

.panelset[
.panel[.panel-name[R code]
```{r eval = FALSE, fig.height=3}
summary(lmod_putts)
anova(lmod_putts, test = "Chisq")
```
]
.panel[.panel-name[summary output]
```{r, echo = FALSE}
summary(lmod_putts)
```

]
.panel[.panel-name[anova output]
```{r echo=FALSE}
anova(lmod_putts, test = "Chisq")
```
]
]


---

###  Drop in Deviance (G) Test: Putts2 (aggregate) data

.panelset[
.panel[.panel-name[R code]
```{r eval = FALSE, fig.height=3}
summary(lmod_Putts2)
anova(lmod_Putts2, test = "Chisq")
```
]
.panel[.panel-name[summary output]
```{r, echo = FALSE}
summary(lmod_Putts2)
```

]
.panel[.panel-name[anova output]
```{r echo=FALSE}
anova(lmod_Putts2, test = "Chisq")
```
]
]


---

### Test for Overall Model: Logistic Regression

**Drop-in-Deviance Test**

- Is there something effective in the model?

.pull-left[

- **Null hypothesis**

  - $H_0:\beta_1=0$
  
  - $\log\left(\frac{\pi}{1-\pi}\right)=\beta_0$
  
  - Same odds for all values of $x$
  
  
- **Alternative hypothesis**

  - $H_0: \beta_1\neq 0$
  
  - $\log\left(\frac{\pi}{1-\pi}\right)=\beta_0+\beta_1 x$
  
  - Odds are a linear function of $x$
  
]

--

.pull-right[

- $G=-2\log(L_0)-(-2\log(L))$ and compare to $\chi^2_1$

  - Improvement in $-2\log(L)$ when using linear function of $x$.
  
]