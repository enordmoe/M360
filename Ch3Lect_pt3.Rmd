---
title: "Chapter 3: Inference for Simple Linear Regression"
subtitle: "Part 3: Section 3.4-3.5"
date: "Applied Statistics II"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "extra.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---
class: highlight-last-item

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width=9, fig.height=4,  
  out.width="90%",
  message = FALSE
)

```



```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(mosaic)
library(Stat2Data)
library(gridExtra)
library(plotly)
library(readr)
funneldata <- read_csv("http://people.kzoo.edu/enordmoe/math360/funneldata.csv", col_types = cols(trial = col_factor(levels = c("1","2"))))
funnel.hi <- filter(funneldata, height == 13)
funnel.hilo <- filter(funneldata, height == 13 | height == 10)
funnel.hilo <- mutate(funnel.hilo, height.hi = ifelse(height==13,1,0))
funneldata <- mutate(funneldata, height.hi = ifelse(height==13, 1, 0),
                   height.mid = ifelse(height == 11.5, 1, 0))
data("Perch")

```

```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_xaringan_extra(c("panelset"))
xaringanExtra::use_clipboard()
xaringanExtra::use_tile_view()
```


```{css echo=FALSE}
.highlight-last-item > ul > li,
.highlight-last-item > ol > li {
  opacity: 0.5;
}
.highlight-last-item > ul > li:last-of-type,
.highlight-last-item > ol > li:last-of-type {
  opacity: 1;
}
```

# Outline 

- Interaction model

--

- Polynomial regression

  - Quadratic model
  
--
  
- Second-order model

--

- Multicollinearity

  - Variance Inflation Factor (VIF)

---

## Interaction

Recall the funnel data model for two regression lines:

$$
\text{tswirl} = \beta_0 + \beta_1 (\text{dist}) + \beta_2 (\text{funnel.hi}) + \beta_3 (\text{dist}\cdot \text{funnel.hi}) + \epsilon
$$

- The product term $\beta_3 (\text{dist}\cdot \text{funnel.hi})$ allows for different effects of distance for low and high settings.

--

<br>

**Interaction:** When the relationship between two variables changes depending on a third variable.

  - Consider adding a product term to account for interactions where the context suggests it.

---

## Fish Weight Analysis
### Example 3.11

- **Dataset:** **.red[Perch]** (measurements for 56 fish)

  - **Predictors:** `Length`, `Width` (in cm)
  
  - **Response:** `Weight` (in gm)

<br>

* Fit a two-predictor model with an interaction.

* Question: Why might the product of the two explanatory variables be important here? What does the product represent?

---
### Example R Code for **.red[Perch]** Data


.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
data("Perch")
perch_mod <- lm(Weight ~ Length + Width + I(Length * Width), data = Perch)
summary(perch_mod)
```
]

---

### Example R Code for **.red[Perch]** Data

Points to note:

- Use of `I()` to create product term without creating a new variable.

- Interpretation of coefficient of interaction.

  - Compare "effect" of an increase in `Width` for different values of `Length`
  
  - For example: write the model for different values of `Length` (e.g., 20 cm and 40 cm)
  
  - Note difference in effect of an extra 1cm in `Width`.
  
---
### Example R Code for **.red[Perch]** Data

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
anova(perch_mod)
```
]

---
### Example R Code for **.red[Perch]** Data

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
gf_point(resid(perch_mod) ~ fitted(perch_mod)) %>%
  gf_hline(yintercept = ~0, col = "red")
```
]

-  Consider `log(Weight)` given non-constant variance.

---

### Example: State SAT Scores

**Response variable $(Y)$:** 
  - `SAT` state average combined SAT score
  
**Potential predictors $(X)$:**

  - `Takers`: % taking SAT exam
  
  - `Expend`: spending per student ($100s)
  
<br>

**Data:** **.red[state_sat]**


---

### Results of Fitting Linear Model

.pull-left[
```{r, echo = FALSE}
state_sat <- read.csv("http://people.kzoo.edu/enordmoe/math360/StateSAT.csv")
gf_point(SAT ~ Takers, data = state_sat) %>%
  gf_smooth(method = "lm", color = "blue") 
```

]

.pull-right[
```{r, echo = FALSE}
satmod_lin <- lm(SAT ~ Takers, data = state_sat)
gf_point(resid(satmod_lin) ~ fitted(satmod_lin)) %>%
  gf_hline(yintercept = ~0, color = "red")
```

]

<br>

$\Longrightarrow$ Consider a "curved" line.

---

## Polynomial Regression

For a single predictor $X$, consider:

$$
\begin{align}
Y &= \beta_0 + \beta_1 X + \beta_2 X^2 + \cdots + \beta_p X^p + \epsilon\\[1ex]
Y &= \beta_0 + \beta_1 X + \epsilon \text{(linear)}\\[1ex]
Y &= \beta_0 + \beta_1 X + \beta_2 X^2 +\epsilon \text{(quadratic)}\\[1ex]
Y &= \beta_0 + \beta_1 X + \beta_2 X^2 +\beta_3 X^3 +\epsilon \text{(cubic)}
\end{align}
$$

<br>

**Caution:** Beware of "overfitting" with large powers $p$.

---

## Fitting a Polynomial Regression in R

**Method 1:** 

  - Use `mutate()` to create new columns with the powers of the predictor variable.

--

**Method 2:** 

  - To avoid creating a new column in the data, use `I()` in the `lm()` specification:

```{r, eval = FALSE}
quadmod <- lm(SAT ~ Takers + I(Takers^2), data = state_sat)
```

---

## Quadratic Model for SAT

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
state_sat <- read.csv("http://people.kzoo.edu/enordmoe/math360/StateSAT.csv")
quadmod <- lm(SAT ~ Takers + I(Takers^2), data = state_sat)
summary(quadmod)
```
]

.red[Fitted model]: $\quad\widehat{\text{SAT}} = 1053.1 -7.161(\text{Takers})+0.0710(\text{Takers}^2)$

---

## SAT data: Plot the Quadratic Model

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
# Use the name of the fitted lm model 
quad_curve <- makeFun(quadmod) 
gf_point(SAT ~ Takers, data = state_sat) %>%
  gf_function(quad_curve, color = "red")
```
]

---

## SAT Data: Residual Plot

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
# Use the name of the fitted lm model 
gf_point(resid(quadmod) ~ fitted(quadmod)) %>%
  gf_hline(yintercept = ~ 0, color = "red")
```
]

---

## Guidelines for Choosing the Polynomial Degree

- Use the minimum degree needed to capture the structure of the data.

- Check the $t$ test of the coefficient for the highest power.

- Keep lower powers even if they are not "significant."

---
background-image: url(https://www.liverpool.ac.uk/pfg/Who/Blog/files/2088_c953_512.gif)
background-size: contain

---

### Complete Second Order Models

**Definition:** A complete second-order model for two predictors would be
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1X_2 + \epsilon
$$

- Allows for curvature in both variables and the possibility of interaction.

- Linear and quadratic models can be obtained by setting certain $\beta_i=0$.

<br>

**Example**: Try a full second-order model for SAT data


---
class: inverse

### Visualizing Second-Order Models

```{r, out.width='65%', fig.align='center', echo = FALSE}
knitr::include_graphics('figures/second_order.png')
```

---

### Example: Second Order Model for SAT Data

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
sat_second <- lm(SAT ~ Takers + Expend + I(Takers^2) 
                 + I(Expend^2) + I(Takers*Expend), data = state_sat)
summary(sat_second)
```
]

---

### Example: Can the model be simplified?

- Do we need the interaction? 

- Do we need both quadratic terms?

- Do we even need the `Expend` terms?  

**Stay tuned** for the nested $F$ test (Section 3.6)

---
class: inverse, middle, center

## Multicollinearity

---

## Mulitcollinearity

- What is it?

--

  - Two or more predictors are strongly associated with each other.
  
--
  
- Why is it a problem?

--

  - Individual coefficients and $t$ tests can be deceptive and unreliable.

--

<br>


**Bad news:** No cures, just treatments!
---

### Effects of Multicollinearity

- If predictors are highly correlated among themselves:

1. The regression coefficients and tests can be extremely variable and difficult to interpret individually.

1. One variable alone might work as well as many.

<br>

- Approach may depend on the goal of the analysis: description vs prediction.

---

## How to Detect Multicollinearity?
### 1. Correlation Matrix

- **Dataset:** **.red[MidtermFinalA]** (class scores)

  - **Response:** `Final` exam score

  - **Predictors:** `Quiz` average, `Class` participation, and `Midterm` score
  
---

### Correlation Matrix for MidtermFinalA

- Note high correlation between `Midterm` and `Quiz`

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
MidtermFinalA <- read.csv("http://people.kzoo.edu/enordmoe/math360/MidtermFinalA.csv")
options(digits = 3)
M <- MidtermFinalA %>%
  select(Midterm, Final, Quiz, Class) %>%
  cor()
M
# or use cor(MidtermFinalA[2:5])
options(digits=5)
```
]

---

### Multicollinearity: Simulation

- Explore stability of regression results in the presence of multicollinearity

  - `Quiz` and `Midterm` are related
  
  - `Class` participation is independent of the others.
  
- Link to download <a href="https://www.dropbox.com/s/w9y31uny2tyejwd/SimMultiFinal.R" download>SimMultiFinal.R</a>

---

## How to Detect Multicollinearity?
### 2. Variance Inflation Factor (VIF)

$$
\text{VIF} = \frac{1}{1-R_i^2} 
$$
where $R_i^2$ is the $R^2$ for the regression predicting variable $X_i$ from the other *predictor variables*.

<br>

**Beware** if $\text{VIF}>5$ $\Longleftrightarrow$ $R_i^2>80\%$.

<br>

- Obtain $\text{VIF}$ using **car** package in R.

---

### Example: MidtermFinalA Dataset

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
finalmodel <- lm(Final ~ Midterm + Quiz + Class, data = MidtermFinalA)
summary(finalmodel)
#install.packages(car)
library(car)
vif(finalmodel)
```
]

---

### Output for Checking VIF calculation

- The calculation agrees up to rounding: 
$\text{VIF} = \frac{1}{1-.948} = 19.23$

.panelset[

```{r, panelset = TRUE, fig.height = 3.5}
summary(lm(Midterm  ~  Quiz + Class, data = MidtermFinalA))
```
]


---

### What to Do if You Have Multicollinearity?

1. Choose a better set of predictors.

1. Eliminate some of the redundant predictors.

1. Combine predictors to create a (possibly weighted) scale.

1. "Ignore" the individual coefficients and tests if prediction is the primary goal.

<br>

- Best remedies depend on the purpose of the analysis.