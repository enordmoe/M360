---
title: "Chapter 1: Simple Linear Regression<br><span style='font-size:70%'>Part 2</span>"
subtitle: "Statistical Modeling"
author: "MATH 360"
format:
  revealjs:
    chalkboard: true
    theme: [default, custom.scss]
    toc: false
    toc-depth: 1
    reveal-options:
      slideNumber: true
    html-math-method: mathjax
    incremental: false
    transition: fade
    preview-links: auto
    notes: true
title-slide-attributes:
  data-background-color: "#EA6820"
from: markdown+emoji
execute:
  echo: true
  cache: false
editor:
  markdown:
    wrap: 72
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(mosaic)
library(Stat2Data)
data("WeightLossIncentive4")
data("SpeciesArea")
data("CountyHealth")
```

## Topics :pushpin:

::: incremental
-   Simple linear model conditions

-   Checking conditions
:::

## The Simple Linear Regression Model with Conditions {.smaller}

-   For a quantitative response variable $Y$ and a single quantitative
    explanatory variable $X,$ the **simple linear regression model** is

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$

where $\epsilon$ follows a normal distribution, that is,
$\epsilon\sim N(0,\sigma_\epsilon)$ and the errors are independent from
one another.

::: fragment
-   Two Parts

    -   The model

    -   The conditions
:::

## The Simple Linear Regression Model Graphically

![](figures/regression_with_normals.png)

## Conditions: LINC-R {.smaller}

### I Mneed a Mnemonic

::: incremental
-   **Linearity**: overall relationship is linear.

-   **Independence**: errors are assumed to be independent of each
    other.

-   **Normality**: unseen errors $\epsilon$ are normally distributed.

    -   Needed for confidence intervals and hypothesis tests based on
        $t$-distribution.

-   **Constant variance** (Uniform spread) variability of response does
    not change as the predictor changes.

-   **Randomness** - data are obtained using a random process.

    -   Sampling model determines scope of inference
:::

## Estimating the Standard Deviation $\sigma_\epsilon$ of the Error Term {.smaller}

::: incremental
-   True model errors $\epsilon$ are unobserved

-   Estimate *unobserved* variability in errors $\sigma_\epsilon$ using
    *observed* variability in residuals $y-\hat y$.

-   The **Standard Error of Regression** is the estimated standard
    deviation of the error term based on the least squares fit to a
    sample of $n$ observations:
:::

::: fragment
$$
\hat\sigma_\epsilon=\sqrt{\frac{\sum(y-\hat y)^2}{n-2}}= \sqrt{\frac{\mbox{SSE}}{n-2}}
$$

-   Note: The divisor $n-2$ is known as the *degrees of freedom* of the
    regression.
:::

## Example {.smaller}

### Predicting Final Score from Midterm Score Revisited

```{r}
MidtermFinal <- read.csv("http://people.kzoo.edu/enordmoe/math360/MidtermFinal.csv")
dim(MidtermFinal)
```

::: fragment
-   First 6 cases

```{r}
head(MidtermFinal)
```
:::

## Using R to Find and Plot the Regression Line {.smaller}

::: panel-tabset
## R Code

```{r lsmod1, eval = FALSE}
# Fit and save the regression model
model1 <- lm(Final ~ Midterm, data = MidtermFinal)
#Obtain regression output including slope and intercept
summary(model1)
# Graph regression line on scatterplot
gf_point(Final ~ Midterm, data = MidtermFinal, 
         xlab = "Midterm Score", ylab = "Final Score") |> 
  gf_smooth(method = "lm")
```

## Regression Output

```{r lsmod2, echo = FALSE}
# Fit and save the regression model
model1 <- lm(Final ~ Midterm, data = MidtermFinal)
#Obtain regression output including slope and intercept
summary(model1)
```

## Scatterplot with Regression Line

```{r lsmod3, echo = FALSE}
# Graph regression line on scatterplot
gf_point(Final ~ Midterm, data = MidtermFinal, 
         xlab = "Midterm Score", ylab = "Final Score") |>
  gf_smooth(method = "lm")
```
:::

## What is in an `lm()`? {.smaller}

| Command                             | Contents                     |
|-------------------------------------|------------------------------|
| `model1$fitted` or `fitted(model1)` | predicted values $\hat y$    |
| `model1$resid` or `resid(model1)`   | residuals $y-\hat y$         |
| `model1$coeff` or `coef(model1)`    | estimated model coefficients |
| `model1$call`                       | the form of the model        |
| `model1$model`                      | data used for the model      |

-   These can be used in calculations and plots.

## Special functions for an `lm()` {.smaller}

| Command                 | Contents                                      |
|-------------------------|-----------------------------------------------|
| `model1`                | call and coefficients                         |
| `summary(model1)`       | lots of stuff                                 |
| `summary(model1)$sigma` | std error of regression $\hat\sigma_\epsilon$ |
| `anova(model1)`         | ANOVA table (Chapter 2)                       |

## 

### Checking Linearity: 1. Scatterplot Examples

::: panel-tabset
## R Code

```{r scat1, eval = FALSE}
# Example 1
gf_point(Final ~ Midterm, data = MidtermFinal) %>%
  gf_smooth(method = "lm")
# Example 2
gf_point(Species ~ Area, data = SpeciesArea) %>%
  gf_smooth(method = "lm")
```

## Roughly Linear Plot

```{r scat2, echo = FALSE}
gf_point(Final ~ Midterm, data = MidtermFinal) %>%
  gf_smooth(method = "lm")
```

## Curved Plot

```{r scat3, echo = FALSE}
gf_point(Species ~ Area, data = SpeciesArea) %>%
  gf_smooth(method = "lm")
```
:::

##  {.smaller}

### Checking Linearity: 2. Plot Residuals vs Predicted

::: panel-tabset
## R Code

```{r rvp1, eval = FALSE}
# Example 1
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_point(resid(m1) ~ fitted(m1)) %>%
  gf_hline(yintercept = ~0, color = "blue")

# Example 2
m2 <- lm(Species ~ Area, data = SpeciesArea)
gf_point(resid(m2) ~ fitted(m2)) %>%
  gf_hline(yintercept = ~0, color = "blue")
```

## Roughly Linear Plot

```{r rvp2, echo = FALSE}
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_point(resid(m1) ~ fitted(m1)) %>%
  gf_hline(yintercept = ~0, color = "blue")
```

## Curved Plot

```{r rvp3, echo = FALSE}
m2 <- lm(Species ~ Area, data = SpeciesArea)
gf_point(resid(m2) ~ fitted(m2)) %>%
  gf_hline(yintercept = ~0, color = "blue")
```
:::

##  {.smaller}

### Checking Constant Variance: 1. Scatterplot

::: panel-tabset
## R Code

```{r, eval = FALSE}
# Example 1
 gf_point(Final ~ Midterm, data = MidtermFinal) |>
  gf_smooth(method = "lm")

# Example 2
gf_point(MDs ~ Hospitals, data = CountyHealth) |>
  gf_smooth(method = "lm")
```

## Constant Variance

```{r , echo = FALSE}
gf_point(Final ~ Midterm, data = MidtermFinal) %>%
  gf_smooth(method = "lm")
```

## Increasing Error Variance

```{r , echo = FALSE}
gf_point(MDs ~ Hospitals, data = CountyHealth) %>%
  gf_smooth(method = "lm")
```
:::

##  {.smaller}

### Checking Constant Variance: 2. Plot of Residuals vs Predictions

::: panel-tabset
## R Code

```{r, eval = FALSE}
# Example 1
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_point(resid(m1) ~ fitted(m1)) %>%
  gf_hline(yintercept = ~0, color = "blue")

# Example 2
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_point(resid(m2) ~ fitted(m2)) %>%
  gf_hline(yintercept = ~0, color = "blue")

```

## Constant Variance

```{r , echo = FALSE}
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_point(resid(m1) ~ fitted(m1)) %>%
  gf_hline(yintercept = ~0, color = "blue")
```

## Increasing Error Variance

```{r , echo = FALSE}
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_point(resid(m2) ~ fitted(m2)) %>%
  gf_hline(yintercept = ~0, color = "blue")
```
:::

##  {.smaller}

### Checking Normality of Errors: 1. Histogram of $y -\hat y$

::: panel-tabset
## R Code

```{r, eval = FALSE}
# Example 1
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_histogram(~resid(m1))
# Example 2
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_histogram(~resid(m2))
```

## Normal Residuals

```{r , echo = FALSE}
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_histogram(~resid(m1))
```

## Nonnormal Residuals

```{r , echo = FALSE}
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_histogram(~resid(m2))
```
:::

##  {.smaller}

### Checking Normality of Errors: 1'. Density Plot of $y-\hat y$

::: panel-tabset
## R Code

```{r, eval = FALSE}
# Example 1
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_density(~resid(m1))
# Example 2
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_density(~resid(m2))
```

## Normal Residuals

```{r , echo = FALSE}
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_density(~resid(m1))
```

## Nonnormal Residuals

```{r , echo = FALSE}
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_density(~resid(m2))
```
:::

##  {.smaller}

### Checking Normality of Errors: 2. Normal Quantile Plots of $y-\hat y$

::: panel-tabset
## R Code

```{r, eval = FALSE}
# Example 1
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_qq(~resid(m1)) %>%
  gf_qqline()
# Example 2
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_qq(~resid(m2)) %>%
  gf_qqline()
```

## Normal Residuals

```{r , echo = FALSE}
m1 <- lm(Final ~ Midterm, data = MidtermFinal)
gf_qq(~resid(m1)) %>%
  gf_qqline()
```

## Nonnormal Residuals

```{r , echo = FALSE}
m2 <- lm(MDs ~ Hospitals, data = CountyHealth)
gf_qq(~resid(m2)) %>%
  gf_qqline()
```
:::

## Checking Randomness and Independence {.smaller}

::: incremental
-   Typically cannot be checked by inspection of the data

-   Independence violations

    -   Time series data

    -   Multiple observations from the same family, school, classroom,
        $\ldots$

-   Randomness violations affect scope of inference

    -   Nonrandom sampling

    -   Experimental design
:::

## Summary

-   Regression model conditions: LINC-R

-   Carry out checks of model conditions using R

-   What to do if conditions are not satisfied?

::: fragment
      
:point_right: Stay tuned!

:::
