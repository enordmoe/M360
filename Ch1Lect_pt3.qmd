---
title: "Chapter 1: Simple Linear Regression<br><span style='font-size:70%'>Part 3</span>"
subtitle: "Statistical Modeling"
author: "MATH 360"
format:
  revealjs:
    chalkboard: true
    theme: [default, custom.scss]
    toc: false
    toc-depth: 1
    reveal-options:
      slideNumber: true
    html-math-method: mathjax
    incremental: false
    transition: fade
    preview-links: auto
    notes: true
title-slide-attributes:
  data-background-color: "#EA6820"
from: markdown+emoji
execute:
  echo: true
  cache: false
editor:
  markdown:
    wrap: 72
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(mosaic)
library(Stat2Data)
library(gridExtra)
library(plotly)
data("SpeciesArea")
data("CountyHealth")
data("LongJumpOlympics")
MidtermFinal <- read.csv("http://people.kzoo.edu/enordmoe/math360/MidtermFinal.csv")
data("WeightLossIncentive4")
```

## Topics :pushpin:

-   Transformations

-   Outliers and Influential Points

------------------------------------------------------------------------

### What to Do When Regression Conditions Are Violated

Possible issues to address:

::: incremental
1.  Lack of normality in residuals

2.  Patterns in residuals

3.  Heteroscedasticity (nonconstant variability of errors)

4.  Outliers: influential points, large residuals
:::

# Data Transformations {background-color="#FAD9C7"}

## Possible Goals of Data Transformations

::: incremental
1.  Address nonlinear patterns

2.  Stabilize variance

3.  Remove skewness from residuals

4.  Minimize effects of outliers
:::

##  {.smaller}

### Common Transformations

|     Name       |     Mathematically     |
|:---------------|:----------------------:|
| Logarithm      | $Y\rightarrow\log(Y)$  |
| Square root    | $Y\rightarrow\sqrt{Y}$ |
| Exponential    |   $Y\rightarrow e^Y$   |
| Power function |   $Y\rightarrow Y^k$   |
| Reciprocal     |   $Y\rightarrow 1/Y$   |

::: fragment
-   Transformations can be applied to either response $(Y)$ or
    explanatory $(X)$ variables.

-   Both a science and an art

-   Often requires trial and error
:::

##  {.smaller}

### Examples: Doctors and Hospitals in Counties

::: panel-tabset
## R Code

```{r}
#| eval: false
# plot with smoothing line
gf_point(MDs ~ Hospitals, data = CountyHealth) |>
        gf_smooth()
# fit linear regression
mod1 <- lm(MDs ~ Hospitals, data=CountyHealth)
mod1
# scatterplot with line
gf_point(MDs ~ Hospitals, data = CountyHealth) |>
        gf_smooth(method="lm")
```

## Smoothed Plot

```{r}
#| echo: false
gf_point(MDs ~ Hospitals, data = CountyHealth) |>
        gf_smooth()
```

## Output

```{r}
#| echo: false
# fit linear regression
mod1 <- lm(MDs ~ Hospitals, data=CountyHealth)
mod1
```

## Plot with Reg Line

```{r}
#| echo: false
# scatterplot with line
gf_point(MDs ~ Hospitals, data = CountyHealth) |>
        gf_smooth(method="lm")
```
:::

## Linear Model

The fitted linear model is

$$
\widehat{MDs} = -1120.6 + 557.3 \mbox{ Hospitals}
$$

-   Predict number of MDs in a county with 10 hospitals.

## Residual Diagnostics

::: panel-tabset
## R Code

```{r}
#| eval: false
# Residual vs fitted plot
gf_point(resid(mod1) ~ fitted(mod1)) |>
  gf_hline(yintercept = ~0, col = "red")
# Normality plots
# install.packages("gridExtra")
# Use the grid.arrange() function to condense and arrange plots
library(gridExtra)
plot1 <- gf_histogram(~resid(mod1))
plot2 <- gf_qq(~resid(mod1)) |>
  gf_qqline()
grid.arrange(plot1, plot2, ncol=2)
```

## Residual vs Fitted

```{r}
#| echo: false
gf_point(resid(mod1) ~ fitted(mod1)) |>
  gf_hline(yintercept = ~0, col = "red")
```

## Normality plots

```{r}
#| echo: false
plot1 <- gf_histogram(~resid(mod1))
plot2 <- gf_qq(~resid(mod1)) |>
  gf_qqline()
grid.arrange(plot1, plot2, ncol=2)
```
:::

##  {.smaller}

### Transformed model plots

-   Consider whether square root or log transformation of the number of
    MDs results in a better fit.

::: panel-tabset
### R Code

```{r}
#| eval: false
# Transform data
CountyHealth <- mutate(CountyHealth, SqrtMDs = sqrt(MDs), logMDs= log(MDs))
# Create and save plots
plot1 <- gf_point(SqrtMDs ~ Hospitals, data = CountyHealth) |>
  gf_smooth()
# Display plots concisely
plot2<-gf_point(logMDs ~ Hospitals, data = CountyHealth) |>
  gf_smooth()
grid.arrange(plot1,plot2,ncol=2)
```

### Scatterplots

```{r}
#| echo: false
#| fig-height: 3
CountyHealth <- mutate(CountyHealth, SqrtMDs = sqrt(MDs), logMDs= log(MDs))
plot1 <- gf_point(SqrtMDs ~ Hospitals, data = CountyHealth) |>
  gf_smooth()
plot2<-gf_point(logMDs ~ Hospitals, data = CountyHealth) |>
  gf_smooth()
grid.arrange(plot1,plot2,ncol=2)
```
:::

##  {.smaller}

### Square Root Transformation

-   Try the $\sqrt{Y}$ transformation and fit the equation.

-   The red line is the least squares regression line.

::: panel-tabset
### R Code

```{r}
#| eval: false
# Plot smoother and square root model
gf_point(SqrtMDs ~ Hospitals, data=CountyHealth) |>
  gf_smooth(method = "lm", col = "red") |>
  gf_smooth()
# Fit and print square root model
mod2 <- lm(SqrtMDs ~ Hospitals, data = CountyHealth)
mod2
```

### Scatterplot

```{r}
#| echo: false
#| fig-height: 3
gf_point(SqrtMDs ~ Hospitals, data=CountyHealth) |>
  gf_smooth(method = "lm", col = "red") |>
  gf_smooth()
```

### Output

```{r}
#| echo: false
mod2 <- lm(SqrtMDs ~ Hospitals, data = CountyHealth)
mod2
```
:::

##  {.smaller}

### Re-assess conditions

::: panel-tabset
### R Code

```{r}
#| eval: false
# Residual vs fitted plot
gf_point(resid(mod2) ~ fitted(mod2)) |>
  gf_hline(yintercept = ~0)
# Normality plots
# install.packages("gridExtra")
library(gridExtra)
plot1 <- gf_histogram(~resid(mod2))
plot2 <- gf_qq(~resid(mod2)) |>
  gf_qqline()
grid.arrange(plot1, plot2, ncol=2)
```

### Residual vs Fitted

```{r}
#| echo: false
gf_point(resid(mod2) ~ fitted(mod2)) |>
  gf_hline(yintercept = ~0)
```

### Normality plots

```{r}
#| echo: false
plot1 <- gf_histogram(~resid(mod2))
plot2 <- gf_qq(~resid(mod2)) |>
  gf_qqline()
grid.arrange(plot1, plot2, ncol=2)
```
:::

## Fitted model with square root transformation

-   Transformed model is satisfactory.

-   The fitted equation is now $$
    \widehat{\sqrt{\mbox{MDs}}} = -2.753  + 6.876 \mbox{ Hospitals}
    $$

## Questions :clipboard:

1.  Predict the number of MDs in a county with 10 hospitals.

2.  Interpret the slope coefficient.

# Southeast Island Mammal Species {background-color="#FAD9C7"}

------------------------------------------------------------------------

## Southeast Asian Islands

![](https://assets.vacationstogo.com/images/maps/SoutheastAsiaIslands.gif)

## Example: Indonesian Wildlife

::::: columns
::: {.column width="50%"}
Bako Park ![](figures/Bako_park.jpeg)
:::

::: {.column width="50%"}
Green turtles ![](figures/green_turtle.jpeg)
:::
:::::

## Proboscis Monkey {background-image="figures/proboscis_unsplash.jpg"}

## Species vs Area {.smaller}

-   Investigate the dependence of number of species on the size of the
    island.

-   Doesn't make sense that species would be a *linear* function of
    area.

```{r}
#| output-location: fragment
gf_point(Species ~ Area, data = SpeciesArea)
```

##  {.smaller}

### Transformation Consideration

-   Address the strong curvature and extreme values using a natural log
    transformation. Consider the following candidates:

::: panel-tabset
## Code

```{r}
#| eval: false
#| fig-height: 3.5
p1 <- gf_point(Species ~ Area, data = SpeciesArea)
p2 <- gf_point(logSpecies ~ Area, data = SpeciesArea)
p3 <- gf_point(Species ~ logArea, data = SpeciesArea)
p4 <- gf_point(logSpecies ~ logArea, data = SpeciesArea)
grid.arrange(p1,p2,p3,p4,ncol=2,nrow=2)
```

## Output

```{r}
#| echo: false
#| fig-height: 3.5
p1 <- gf_point(Species ~ Area, data = SpeciesArea)
p2 <- gf_point(logSpecies ~ Area, data = SpeciesArea)
p3 <- gf_point(Species ~ logArea, data = SpeciesArea)
p4 <- gf_point(logSpecies ~ logArea, data = SpeciesArea)
grid.arrange(p1,p2,p3,p4,ncol=2,nrow=2)
```
:::

## Fit the Best Model

-   Logging both response and predictor variables appears to give the
    best results.

-   Proceed by fitting a simple regression using the *log-transformed*
    variables.

## Fitting the Log-Log Model {.smaller}

::: panel-tabset
## Code

```{r}
#| eval: false
# Fit the log-log model and get output
mod3 <- lm(logSpecies ~ logArea, data = SpeciesArea)
mod3
# Scatterplot and regression line with transformed variables
gf_point(logSpecies ~ logArea, data = SpeciesArea) |>
        gf_smooth(method="lm")
```

## Regression Output

```{r}
#| echo: false
# Fit the log-log model and get output
mod3 <- lm(logSpecies ~ logArea, data = SpeciesArea)
mod3
```

## Scatterplot with Line

```{r}
#| echo: false
gf_point(logSpecies ~ logArea, data = SpeciesArea) |>
        gf_smooth(method="lm")
```
:::

##  {.smaller}

### The fitted log-log model for predicting number of species

$$
  \widehat{\log(\mbox{Species}}) = 1.625 + 0.2355 \cdot \log(\mbox{Area})
$$

-   Use the model to obtain the predicted value and residual for Java.

-   Interpret the slope coefficient in context.

::: fragment
-   Back transform to get the prediction on the original scale: $$
    \widehat{\mbox{Species}} = e^{1.625 + 0.2355 \cdot \log(\mbox{Area})} =
    5.08\cdot \mbox{Area}^{0.235}
    $$
:::

##  {.smaller}

### Re-assess log-log model

::: panel-tabset
## Code

```{r}
#| eval: false
p1 <- gf_point(resid(mod3) ~ fitted(mod3)) |>
  gf_hline(yintercept = ~0)
p2 <- gf_qq(~resid(mod3)) |>
  gf_qqline()
grid.arrange(p1,p2,ncol=2)
```

## Output

```{r}
#| echo: false
p1 <- gf_point(resid(mod3) ~ fitted(mod3)) |>
  gf_hline(yintercept = ~0)
p2 <- gf_qq(~resid(mod3)) |>
  gf_qqline()
grid.arrange(p1,p2,ncol=2)
```
:::

## 

#### Using R to get Java predictions from the log-log model :clipboard:

-   Sample code:

```{r}
#| eval: false
# Get the actual Java values
filter(SpeciesArea, Name == "Java")  |>
  select(c(Name, Area, Species))
# Obtain the predictions
1.625 + 0.2355 * log(125628)  #Fitted on the log scale
exp(1.625 +0.2355 * log(125628)) #Fitted on the original scale
# or create a function
island_fun <- makeFun(mod3)
exp(island_fun(log(125628)))
# or use the R predict function with "newdata" key
predict(mod3, newdata = data.frame(logArea = log(125628)))
exp(predict(mod3, newdata = data.frame(logArea = log(125628))))
```

-   Try this for a different island :white_check_mark:

# Outliers and Influential Observations {background-color="#FAD9C7"}

## Olympic Long Jump Data

-   Gold medal distance as a function of time:

```{r}
#| output-location: fragment
#| fig-height: 3.5
data(LongJumpOlympics)
gf_point(Gold ~ Year, data = LongJumpOlympics) |>
  gf_smooth() |>
  gf_smooth(method = "lm", color = "red")
```

::: fragment
:question: What do you think has happened since 2008?
:::

##  {.smaller}

### Fit model and identify outlier:

::: panel-tabset
## R Code

```{r}
#| eval: false
#| fig-height: 3.5
mod4 <- lm(Gold ~ Year, data = LongJumpOlympics)
mod4
gf_point(Gold ~ Year, data = LongJumpOlympics) |>
  gf_smooth(method="lm") |>
  gf_refine(annotate("text", label = "Bob Beamon", x = 1968, y = 8.9, size = 6, colour = "red", hjust = -0.1))
```

## Regression Output

```{r}
#| echo: false
mod4 <- lm(Gold ~ Year, data = LongJumpOlympics)
mod4
```

## Scatterplot with Outlier

```{r}
#| echo: false
#| fig-height: 3.5
gf_point(Gold ~ Year, data = LongJumpOlympics) |>
  gf_smooth(method="lm") |>
  gf_refine(annotate("text", label = "Bob Beamon", x = 1968, y = 8.9, size = 6, colour = "red", hjust = -0.1))
```
:::

##  {.smaller}

### Bob Beamon's record-breaking long jump

{{< video https://youtu.be/DEt_Xgg8dzc >}}

##  {.smaller}

### We can use the package **plotly** to identify points interactively (optional)

::: panel-tabset
## Code

```{r}
#| eval: false
library(plotly)
p <- gf_point(Gold ~ Year, data = LongJumpOlympics)
ggplotly(p)
```

## Output

```{r}
#| echo: false
library(plotly)
p <- gf_point(Gold ~ Year, data = LongJumpOlympics)
ggplotly(p)
```
:::

##  {.smaller}

### Obtain the plot of residual versus fitted values

```{r}
#| output-location: fragment
gf_point(resid(mod4)~fitted(mod4)) |>
  gf_hline(yintercept = ~0, color = "red")
```

## {.smaller}
### Other types of residuals 

Scaled residuals are helpful in identifying outliers:

1.  **Standardized residuals.** Residuals divided by estimated standard
    deviation of residuals:

$$
\frac{y - \hat y}{\hat\sigma_\epsilon}
$$

::: fragment
2.  **Studentized residuals.** Residuals divided by standard deviation
    of residuals from the regression *after omitting the point in
    question*

-   Removes effect of extreme observation on standard deviation

-   Implicitly involves fitting $n$ different regression models.

$$
\frac{y - \hat y}{\hat\sigma_i}
$$
:::

## Why studentize?

::: incremental

-   **Concern**: An unusual value may exert great *influence* on the
    fit.

    -   Its residual might be underestimated because the model "moves" a
        lot to fit it. or

    -   The standard error of regression may be *inflated* due to the
        large outlier residual

-   **Remedy**: We **studentize** by fitting the model **without** the
    influential case, finding a new estimate $\widehat{\sigma_\epsilon}$.

:::

##  {.smaller}
### Plots of standardized and studentized residuals versus fitted

::: panel-tabset

## Code

```{r lj4c}
#| eval: false
#| fig-height: 4.5
#Standardized resids
p1 <- gf_point(rstandard(mod4) ~ Year, data = LongJumpOlympics, ylim = c(-4,4),
               main="Standardized Residuals") |>
  gf_hline(yintercept = ~0) |>
  gf_hline(yintercept = ~c(-3,-2,2,3), lty = 2, data = NA)
#Studentized resids
p2 <- gf_point(rstudent(mod4) ~ Year, data = LongJumpOlympics, ylim = c(-4,4),
               main="Studentized Residuals") |>
  gf_hline(yintercept = ~0) |>
  gf_hline(yintercept = ~c(-3,-2,2,3), lty = 2, data = NA)
grid.arrange(p1 ,p2, ncol=2)
```

## Output

```{r lj4o}
#| echo: false
#| fig-height: 4.5
#Standardized resids
p1 <- gf_point(rstandard(mod4) ~ Year, data = LongJumpOlympics, ylim = c(-4,4),
               main="Standardized Residuals") |>
  gf_hline(yintercept = ~0) |>
  gf_hline(yintercept = ~c(-3,-2,2,3), lty = 2, data = NA)
#Studentized resids
p2 <- gf_point(rstudent(mod4) ~ Year, data = LongJumpOlympics, ylim = c(-4,4),
               main="Studentized Residuals") |>
  gf_hline(yintercept = ~0) |>
  gf_hline(yintercept = ~c(-3,-2,2,3), lty = 2, data = NA)
grid.arrange(p1 ,p2, ncol=2)
```
:::

## <span class="red"> Beamon is an extreme outlier!</span>

![](https://e0.365dm.com/21/07/768x432/skysports-bob-beamon-olympics_5462311.jpg)


## Example: The Butterfly Ballot in the Year 2000 Election

::::: columns
::: {.column width="50%"}
![](https://upload.wikimedia.org/wikipedia/commons/4/4e/Butterfly_Ballot%2C_Florida_2000_%28large%29.jpg)
:::

::: {.column width="50%"}
![](https://media.npr.org/assets/img/2018/11/12/gettyimages-1306049_wide-78b1a96ccca5fcaac551979e4a29942620227e3d.jpg?s=1400)
:::
:::::


## Plot the Votes by County


```{r pres1}
#| message: false
#| warning: false
#| output-location: fragment
data("PalmBeach")
p <- gf_point(Buchanan ~ Bush, label = ~County, data = PalmBeach)
ggplotly(p)
```


## {.smaller} 
### Linear model fit and plots

::: panel-tabset
### R Code

```{r pres3}
#| eval: false
# Fit and display model
mod5 <- lm(Buchanan ~ Bush, data = PalmBeach)
# Regresssion line plot
gf_point(Buchanan ~ Bush, data = PalmBeach) |>
  gf_smooth(method = "lm")
# Residual plot
gf_point(resid(mod5) ~ fitted(mod5)) |>
  gf_hline(yintercept = ~0, color = "blue")
```

### Plot with Regression Line

```{r}
#| echo: false
# Fit and display model
mod5 <- lm(Buchanan ~ Bush, data = PalmBeach)
# Regresssion line plot
gf_point(Buchanan ~ Bush, data = PalmBeach) |>
  gf_smooth(method = "lm")
```

### Residual vs Fitted Plot

```{r}
#| echo: false
gf_point(resid(mod5) ~ fitted(mod5)) |>
  gf_hline(yintercept = ~0, color = "blue")
```
:::

## Is Palm Beach an Influential Outlier?

```{r pres4}
#| output-location: fragment
# Omit Palm Beach
PalmBeach_noPB <- filter(PalmBeach, County != "PALM BEACH")
# Refit the regression without Palm Beach
mod6 <- lm(Buchanan ~ Bush, data = PalmBeach_noPB)
mod6
mod6_fun <- makeFun(mod6)
```


## Is Palm Beach an Influential Outlier?

```{r pres5}
#| message: false
#| warning: false
#| fig-height: 3
gf_point(Buchanan ~ Bush, data = PalmBeach) |>
        gf_smooth(method = "lm", se = FALSE) |>
  gf_fun(mod6_fun, col = "red") |>
  gf_refine(annotate("text", label = "Complete", x = 2.5e+05, y = 1400, size = 6, hjust = 1)) |>
  gf_refine(annotate("text", label = "Without\nPalm Beach",
                     x = 2e+05, y = 700, size = 6, hjust = 0, vjust = 1))

```

::: aside
Note: The `\n` on line 5 of the code is the "new line" character that causes the label to wrap.
:::


## Influence and Leverage

::: incremental
-   **Influence**: the effect of a single data point on the regression
    line depends on:

    -   how well it matches the "trend" of the rest of the points

    -   how "unusual" is its predictor value

-   **Leverage**: measures the potential for a case to affect a
    regression fit, based on an "unusual" predictor value. (Chapter 4)
:::

## Summary 

- When regression conditions fail, **transforming variables** can improve model fit.

- **Log and square-root transformations** often help with curvature, unequal spread, and skewness.

- **Outliers and influential points** can distort a regression line and must be checked using diagnostics.

- After any adjustment, **reassess conditions and interpret results in context**.
